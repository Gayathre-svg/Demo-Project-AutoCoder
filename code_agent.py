# app/agents/code_agent.py
import os, json, re
from app.openai_client import get_openai_client

def generate_code_files(spec: str, context: str = "") -> dict:
    """
    Returns dict filepath -> content (generated by LLM).
    """
    prompt = f"""
You are an expert Python developer that generates small, well-tested, idiomatic code.
Project context:
{context}

Feature request:
{spec}

Produce:
1) A python module file path and content (relative path under project root).
2) A pytest unit test file path and content that validates the feature.
Return a JSON object with keys: "files" where value is a dict mapping filepath->content.
Only return the JSON (no additional commentary).
"""
    client = get_openai_client()
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role":"system","content":"You are a helpful, concise code generator."},
            {"role":"user","content":prompt},
        ],
        max_tokens=1200,
        temperature=0.2,
    )
    text = resp.choices[0].message.content
    m = re.search(r"(\{[\s\S]*\})", text)
    if not m:
        raise ValueError("LLM did not return JSON. Raw:\n" + text)
    j = json.loads(m.group(1))
    return j.get("files", {})
